{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nucleus Instance Segmentation\n",
    "Click to open in: [[GitHub](https://github.com/TissueImageAnalytics/tiatoolbox/tree/master/examples/08-nucleus-instance-segmentation.ipynb)][[Colab](https://colab.research.google.com/github/TissueImageAnalytics/tiatoolbox/blob/master/examples/08-nucleus-instance-segmentation.ipynb)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3l08vukKAWo",
    "nbsphinx": "hidden"
   },
   "source": [
    "## About this notebook\n",
    "This jupyter notebook can be run on any computer with a standard browser and no prior installation of any programming language is required. It can run remotely over the Internet, free of charge, thanks to Google Colaboratory. To connect with Colab, click on one of the two blue checkboxes above. Check that \"colab\" appears in the address bar. You can right-click on \"Open in Colab\" and select \"Open in new tab\" if the left click does not work for you. Familiarize yourself with the drop-down menus near the top of the window. You can edit the notebook during the session, for example substituting your own image files for the image files used in this demo. Experiment by changing the parameters of functions. It is not possible for an ordinary user to permanently change this version of the notebook on GitHub or Colab, so you cannot inadvertently mess it up. Use the notebook's File Menu if you wish to save your own (changed) notebook.\n",
    "\n",
    "To run the notebook on any platform, except for Colab, set up your Python environment, as explained in the\n",
    "[README](https://github.com/TIA-Lab/tiatoolbox/blob/master/README.md#install-python-package) file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## About this demo\n",
    "Each WSI can contain up to million nuclei of various types, which can be further analysed systematically and used for predicting clinical outcomes. In order to use nuclear features for downstream analysis within computational pathology, nucleus segmentation and classification must be carried out as an initial step. However, this remains a challenge because nuclei display a high level of heterogeneity and there is significant inter- and intra-instance variability in the shape, size and chromatin pattern between and within different cell types, disease types or even from one region to another within a single tissue sample. Tumour nuclei, in particular, tend to be present in clusters, which gives rise to many overlapping instances, providing a further challenge for automated segmentation, due to the difficulty of separating neighbouring instances.\n",
    "\n",
    ">![image](https://tiatoolbox.dcs.warwick.ac.uk/notebook/hovernet_samples.PNG)\n",
    "> Image courtesy of Graham, Simon, et al. \"Hover-net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images.\" Medical Image Analysis 58 (2019): 101563.\n",
    "\n",
    "In this example, we will demonstrate how you can use the TIAToolbox implementation of [HoVer-Net](https://arxiv.org/pdf/1812.06499.pdf) to tackle these challenges and solve the problem of nuclei instance segmentation and classification within histology images. HoVer-Net is a deep learning approach based on horizontal and vertical distances (and hence the name HoVer-Net) of nuclear pixels to the centre of mass of the corresponding nucleus. These distances are used to separate clustered nuclei. For each segmented instance, the nucleus type is subsequently determined via a dedicated up-sampling branch.\n",
    "\n",
    "In this example notebook, we are not going to explain how HoVer-Net works (for more information we refer you to the [HoVer-Net paper](https://www.sciencedirect.com/science/article/pii/S1361841519301045)), but we will show how easily you can use the sophisticated HoVer-Net model, which is incorporated in TIATtoolbox, to do automatic segmentation and classification of nucleus instances. Mostly, we will be working with the [NucleusInstanceSegmentor](https://github.com/TissueImageAnalytics/tiatoolbox/blob/master/tiatoolbox/models/engine/nucleus_instance_segmentor.py#L256) which by default uses one of the pretrained  [HoVerNet](link_to_hovernet_dcoumentation) models. We will also cover the [visualisation tool](https://tia-toolbox.readthedocs.io/en/develop/usage.html?#module-tiatoolbox.utils.visualization) embedded in TIAToolbox for overlaying the instance segmentation results on the input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the environment\n",
    "### TIAToolbox and dependencies installation\n",
    "You can skip the following cell if 1) you are not using the Colab plaform or 2) you are using Colab and this is not your first run of the notebook in the current runtime session. If you nevertheless run the cell, you may get an error message, but no harm will be done. On Colab the cell installs `tiatoolbox`, and other prerequisite software. Harmless error messages should be ignored. Outside Colab , the notebook expects `tiatoolbox` to already be installed. (See the instructions in [README](https://github.com/TIA-Lab/tiatoolbox/blob/master/README.md#install-python-package).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oCOSzUCUXnfh",
    "nbsphinx": "hidden",
    "outputId": "4f06aa6a-69c7-4fb9-b124-d1f10b1527ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Unable to locate package libopenjp2-7-dev\n",
      "E: Unable to locate package libopenjp2-tools\n",
      "E: Unable to locate package openslide-tools\n",
      "Reading state information...\n",
      "\u001b[33mWARNING: jsonschema 4.2.1 does not provide the extra 'format-nongpl'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!apt-get -y install libopenjp2-7-dev libopenjp2-tools openslide-tools | tail --line 1\n",
    "!pip install tiatoolbox | tail --line 1\n",
    "\n",
    "print(\"Installation is done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **IMPORTANT**: If you are using Colab and you run the cell above for the first time, please note that you need to restart the runtime before proceeding through (menu) *\"Runtime&#8594;Restart runtime\"* . This is needed to load the latest versions of prerequisite packages installed with TIAToolbox. Doing so, you should be able to run all the remaining cells altogether (*\"Runtime&#8594;Run after\"* from the next cell) or one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU or CPU runtime\n",
    "Processes in this notebook can be accelerated by using a GPU. Therefore, whether you are running this notebook on your system or Colab, you need to check and specify if you are using GPU or CPU hardware acceleration. In Colab, you need to make sure that the runtime type is set to GPU in the *\"Runtime&#8594;Change runtime type&#8594;Hardware accelerator\"*. If you are *not* using GPU, consider changing the `ON_GPU` flag to `Flase` value, otherwise, some errors will be raised when running the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ON_GPU = True  # Should be changed to False if no cuda-enabled GPU is available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jrx0GDVdXnfg",
    "nbsphinx": "hidden"
   },
   "source": [
    "### Removing leftovers from previous runs\n",
    "The cell below removes some redundant directories if they existâ€”a previous run may have created them. This cell can be skipped if you are running this notebook for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W2-2vEGzXnfg",
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "![ -d tmp ] && ( echo \"deleting tmp directory\"; rm -rf tmp )\n",
    "![ -d sample_tile_results ] && !( echo \"deleting 'sample_tile_results' directory\"; rm -r sample_tile_results)\n",
    "![ -d sample_wsi_results ] && ( echo \"deleting 'sample_wsi_results' directory\"; rm -r sample_wsi_results)\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmJxBFzDJLPj"
   },
   "source": [
    "### Importing related libraries\n",
    "\n",
    "We import some standard Python modules, and also the Python module `wsireader` (see [details](https://github.com/TIA-Lab/tiatoolbox/blob/master/tiatoolbox/wsicore/wsireader.py)) written by the TIA Centre team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEIfjUTaJLPj"
   },
   "outputs": [],
   "source": [
    "from tiatoolbox.models.engine.nucleus_instance_segmentor import NucleusInstanceSegmentor\n",
    "from tiatoolbox.utils.misc import imread\n",
    "from tiatoolbox.wsicore.wsireader import WSIReader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import os, glob\n",
    "\n",
    "mpl.rcParams[\"figure.dpi\"] = 300  # for high resolution figure in notebook\n",
    "plt.rcParams.update({\"font.size\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kA9ti0jNXniD"
   },
   "source": [
    "### Downloading the required files\n",
    "We download, over the internet, image files used for the purpose of this notebook. In particular, we download a histology tile and a whole slide image of cancerous breast tissue samples to show how the nucleus instance segmentation model works.\n",
    "> In Colab, if you click the files icon (see below) in the vertical toolbar on the left hand side then you can see all the files that the code in this notebook can access. The data will appear here when it is downloaded.\n",
    ">\n",
    "> ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACQAAAAlCAYAAAAqXEs9AAAAwElEQVRYhe3WMQ6DMAyFYa7q1Yfw7Dl3ICusZM0hzJpDMLtTGSoFNy2UVPIvvf3DYsignTXcDXjNQVYOsnKQlYOsDkHjOCoiKgBUl3P+DWhZlkPIVagqaJqmt0EAoDFGnefZXEpJt227HtQyZv4chIjKzKeMiHZU7Uom6OhrWhORHSQiDnKQg/oChRD6AjGzg/4L9PyHiEjXdT1lKaUdVEppA7W8h1qHiNUrfv1ibB0RVa9jgu7IQVYOsnKQVXegB/ZWYoL8lUCBAAAAAElFTkSuQmCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rjw14DaJlO_o",
    "outputId": "3a6d2c5c-8682-46d7-8f5d-ff2f3ab54473"
   },
   "outputs": [],
   "source": [
    "from tiatoolbox.utils.misc import download_data\n",
    "\n",
    "# These file name are used for the experimenets\n",
    "img_file_name = \"sample_tile.png\"\n",
    "wsi_file_name = \"sample_wsi.svs\"\n",
    "\n",
    "\n",
    "print(\"Download has started. Please wait...\")\n",
    "\n",
    "# Downloading sample image tile\n",
    "download_data(\n",
    "    \"https://tiatoolbox.dcs.warwick.ac.uk/sample_imgs/breast_tissue_crop.png\",\n",
    "    img_file_name,\n",
    ")\n",
    "\n",
    "# Downloading sample whole-slide image\n",
    "download_data(\n",
    "    \"https://tiatoolbox.dcs.warwick.ac.uk/sample_wsis/wsi4_12k_12k.svs\", wsi_file_name\n",
    ")\n",
    "\n",
    "print(\"Download is complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKterRqUKHnW"
   },
   "source": [
    "## Nucleus instance segmentation and classification using TIAToolbox's pretrained HoVer-Net model\n",
    "In this section, we will investigate the use of the HoVer-Net model that has been already trained on the PanNuke dataset and incorporated in the TIAToolbox.\n",
    "The model we demonstrate can segment out nucleus instances in the image and assign one of the following 5 classes to them:\n",
    "- Neoplastic Epithelial\n",
    "- Non-Neoplastic Epithelial\n",
    "- Inflammatory\n",
    "- Connective\n",
    "- Dead\n",
    "\n",
    "### Inference on tiles\n",
    "Similarly to the semantic segmentation functionality of the TIAToolbox, the instance segmentation module works both on image tiles and structured WSIs. First, we need to create an instance of the  `NucleusInstanceSegmentor` class which controls the  whole process of the nucleus instance segmentation task and then use it to do prediction on the input image(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "irBRlF2_JLPj",
    "outputId": "603d0e17-7f7e-4a77-e837-77434461cc79"
   },
   "outputs": [],
   "source": [
    "# Tile prediction\n",
    "inst_segmentor = NucleusInstanceSegmentor(\n",
    "    pretrained_model=\"hovernet_fast-pannuke\",\n",
    "    num_loader_workers=2,\n",
    "    num_postproc_workers=2,\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "tile_output = inst_segmentor.predict(\n",
    "    [img_file_name],\n",
    "    save_dir=\"sample_tile_results/\",\n",
    "    mode=\"tile\",\n",
    "    on_gpu=ON_GPU,\n",
    "    crash_on_exception=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7JkXBtAETIn"
   },
   "source": [
    "There we go! With only two lines of code, thousands of images can be processed automatically.\n",
    "There are various parameters associated with `NucleusInstanceSegmentor`. We explain these as we meet them while proceeding through the notebook. Here we explain only the ones mentioned above:\n",
    "\n",
    "- `pretrained_model`: specifies the name of the pretrained model included in the TIAToolbox (case sensitive). We are expanding our library of models pretrained on various (instance) segmentation tasks. You can find a complete list of currently available pretrained models [here](https://tia-toolbox.readthedocs.io/en/latest/usage.html?highlight=architecture#module-tiatoolbox.models.architecture). In this example, we use the `\"hovernet_fast-pannuke\"` pretrained model, which is HoVer-Net model trained on the [PanNuke dataset](https://arxiv.org/abs/2003.10778). Another option for HoVer-Net is `hovernet_original-kumar` which is the original version of the HoVer-Net model trained on a dataset by [Kumar et al.](https://www.dropbox.com/s/j3154xgkkpkri9w/IEEE_TMI_NuceliSegmentation.pdf?dl=0)\n",
    "\n",
    "- `num_loader_workers`: as the name suggests, this parameter controls the number of CPU cores (workers) that are responsible for the \"loading of network input\" process, which consists of patch extraction, preprocessing, etc.\n",
    "\n",
    "- `num_postproc_workers`: as the name suggests, this parameter controls the number of CPU cores (workers) that are responsible for the \"model related post-processing\" tasks, which consist of horizontal and vertical maps gradient calculations, instance marker generation, and marker-controlled watershed method to generate the final instance map (for more information, please refer to the [HoVer-Net paper](https://www.sciencedirect.com/science/article/pii/S1361841519301045)).\n",
    "\n",
    "- `batch_size`: controls the batch size, or the number of input instances to the network in each iteration. If you use a GPU, be careful not to set the `batch_size` larger than the GPU memory limit would allow.\n",
    "\n",
    "After the `inst_segmentor` has been instantiated as the instance segmentation engine with our desired pretrained model, one can call the `predict` method to do inference on a list of input images (or WSIs). The `predict` function automatically processes all the images on the input list and saves the results on the disk. The process usually comprises patch extraction (because the whole tile or WSI won't fit into limited GPU memory), preprocessing, model inference, post-processing and prediction assembly. Here are some important parameters required by the `predict` method properly:\n",
    "\n",
    "- `imgs`: List of inputs to be processed. Note that items in the list should be paths to the inputs.\n",
    "\n",
    "- `save_dir`: Path to the main folder in which prediction results for each input are stored separately.\n",
    "\n",
    "- `mode`: the mode of inference which can be set to either `'tile'` or `'wsi'` for plain histology images or structured whole slides images, respectively.\n",
    "\n",
    "- `on_gpu`: can be `True` or `False` to dictate running the computations on GPU or CPU.\n",
    "\n",
    "- `crash_on_exception`: If set to `True`, the running loop will crash if there is an error during processing a WSI. Otherwise, the loop will move on to the next image (wsi) for processing. We suggest that you first make sure that the prediction is working as expected by testing it on a couple of inputs and then set this flag to `False` to process large cohorts of inputs.\n",
    "\n",
    "In `tile_output`, the `prediction` method returns a list of paths to its inputs and to the processed outputs saved on disk. This can be used later during processing and visualisation.\n",
    "Similarly to prediction for the `SemanticSegmentor` class, the `predict` method here can also accepts some arguments to set the input/output configurations of the model. These arguments are `ioconfig` (which accecpts a instance from `tiatoolbox.models.engine.semantic_segmentor.IOSegmentorConfig` class) or `resolution`, `patch_input_shape`, `patch_output_shape`, and `stride_shape` based on which an appropriate `IOSegmentorConfig` will be generated.\n",
    "When you are using TIAToolbox pretrained models, we recommend using the default values of input/output shape.\n",
    "\n",
    "Now that the prediction has finished, let's use the paths in `tile_output` to load and examine the predictions. For that, we need to use `joblib` package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "tile_preds = joblib.load(f\"{tile_output[0][1]}.dat\")\n",
    "print(\"Number of detected nuclei: {}\".format(len(tile_preds)))\n",
    "\n",
    "# Extracting the nucleus IDs and select the first one\n",
    "nuc_id_list = list(tile_preds.keys())\n",
    "selected_nuc_id = nuc_id_list[0]\n",
    "print(\"Nucleus prediction structure for nucleus ID: {}\".format(selected_nuc_id))\n",
    "sample_nuc = tile_preds[selected_nuc_id]\n",
    "print(sample_nuc.keys())\n",
    "print(\"Bounding box: {}\".format(sample_nuc[\"box\"]))\n",
    "print(\"Centroid: {}\".format(sample_nuc[\"centroid\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diGJJFn8pvRf"
   },
   "source": [
    "After loading the prediction for the first image in the `tile_output` list (there is only one image or path in this example), we can see that the nucleus predictions for that image are stored in a Python Dictionary each with a unique dictionary key which we will call `nuc_id` from now on. Here is the dictionary format:\n",
    "```\n",
    "sample_dict = {nuc_id: {\n",
    "                             box: List[],\n",
    "                             centroid: List[],\n",
    "                             contour: List[List[]],\n",
    "                             prob: float,\n",
    "                             type: int\n",
    "                             }\n",
    "                ... # other instances\n",
    "              }\n",
    "```\n",
    "\n",
    "One can extract the list of all nucleus IDs by calling `nuc_id_list = list(tile_preds.keys())`. The value for each `nuc_id` is a dictionary itself which contains information related to that particular nucleus instance. Each nucleus instance dictionary consists the following information (keys):\n",
    "- `'box'`: Bounding box information in the $[x_{top-left}, y_{top-left}, width, hight]$ format.\n",
    "- `'centroid'`: Centroid of the nucleus in $[x_{centre}, y_{centre}]$ format.\n",
    "- `'contour'`: A list of points that form the contour or segmentation boundary (polygon) of the nucleus. This can be used to regenerate the nucleus mask.\n",
    "- `'prob'`: The probability of type prediction.\n",
    "- `'type'`: Predicted type or label for the nucleus which can be an integer between 0 and 5: {0: 'neoplastic epithelial', 1: 'Inflammatory', 2: 'Connective', 3: 'Dead', 4: 'non-neoplastic epithelial'}\n",
    "\n",
    "### Instance Segmentation Visualisation\n",
    "Investigating the quality of instance segmentation/classification by going through the predicted nucleus dictionary, as explained above, is impossible. A good workaround is to visualize the instance segmentation results on the input image to check the quality of instance segmentation and classification. To this end, TIAToolbox has incorporated a boundary visualisation tool, called `overlay_prediction_contours`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 711
    },
    "id": "01xXoAgIvByb",
    "nbsphinx-thumbnail": {
     "output-index": -1
    },
    "outputId": "4ee1c3cb-18cf-49c0-d899-b66d499a9208"
   },
   "outputs": [],
   "source": [
    "from tiatoolbox.utils.visualization import overlay_prediction_contours\n",
    "\n",
    "# Reading the original image\n",
    "tile_img = imread(img_file_name)\n",
    "\n",
    "# defining the coloring dictionary: a dictionary that specifies a color to each class {type_id : (type_name, colour)}\n",
    "color_dict = {\n",
    "    0: (\"neoplastic epithelial\", (255, 0, 0)),\n",
    "    1: (\"Inflammatory\", (255, 255, 0)),\n",
    "    2: (\"Connective\", (0, 255, 0)),\n",
    "    3: (\"Dead\", (0, 0, 0)),\n",
    "    4: (\"non-neoplastic epithelial\", (0, 0, 255)),\n",
    "}\n",
    "\n",
    "# Create the overlay image\n",
    "overlaid_predictions = overlay_prediction_contours(\n",
    "    canvas=tile_img,\n",
    "    inst_dict=tile_preds,\n",
    "    draw_dot=False,\n",
    "    type_colours=color_dict,\n",
    "    line_thickness=2,\n",
    ")\n",
    "\n",
    "# showing processed results alongside the original images\n",
    "fig = plt.figure()\n",
    "ax1 = plt.subplot(1, 2, 1), plt.imshow(tile_img), plt.axis(\"off\")\n",
    "ax2 = plt.subplot(1, 2, 2), plt.imshow(overlaid_predictions), plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xsu3RMP9ILpA"
   },
   "source": [
    "As you can see, `overlay_prediction_contours` beautifully generates an image that has the instance segmentation classification overlaid. Here are the explanations regarding this function's arguments:\n",
    "\n",
    "- `canvas`: the image on which we would like to overlay the predictions. This is the same image as the input to the `predict` method, loaded as a numpy array.\n",
    "- `inst_dict`: predicted instance dictionary. This is the dictionary that we earlier loaded using the `joblib.load()` function. It is the output of HoVer-Net and contains the predicted instances.\n",
    "- `draw_dot`: specifies whether to show detected nucleus centroids on the overlap map. Default is False.\n",
    "- `type_colours`: a dictionary containing the name and colour information for each class in the prediction. The HoVer-Net model in this example predicts 5 classes of cell, and so we created  `color_dict` containing colour information for all types. The `type_colours` dictionary uses the formal `{type_id : (type_name, colour)}` format where `type_id` is from 0 to N (corresponding to classes) and `colour` is a tuple (R, G, B) specifying the colour in RGB format.\n",
    "- `inst_colours`: if you only have the instance segmentation results or do not like to colour instances based on their types, you can set this parameter to assign a colour for all instances or provide a list of colours to assign different colours to different instances in `inst_dict`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2Fu6C0-2ysd"
   },
   "source": [
    "### Inference on WSIs\n",
    "The next step is to use TIAToolbox's embedded model for nucleus instance segmentation on a whole slide image. The process is quite similar to what we have done for tiles. We will just introduce  some important parameters that configure the instance segmentor for WSI inference.\n",
    "\n",
    "Here we re-defined the `inst_segmentor` just to show the use of the `auto_generate_mask` parameter. By setting this parameter to `True`, we are telling TIAToolbox to automatically extract the tissue masks of the input WSIs if they are not provided in the `predict` function parameters.\n",
    "In this example, we leave `auto_generate_mask=False` because we are using a WSI that contains only tissue region (there is no background region) and therefore there is no need for tissue mask extraction.\n",
    "> Please note that this part may take too long to process, depending on the system you are using (GPU enabled/disabled) and how large the input WSI is."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "collapsed_sections": [],
   "name": "08-nucleus-instance-segmentation.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "641317031b9f54126a387462bfafcc8a3ed3bbe28fae551a718f7a4af76cf9e8"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
